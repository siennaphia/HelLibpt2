{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openfhe.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libOPENFHEpke.1.dylib\n  Referenced from: <5B46D50D-E5D8-3708-B922-EB28A1D0814B> /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openfhe.cpython-312-darwin.so\n  Reason: no LC_RPATH's found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnaive_regression\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrypto_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_crypto\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnaive_regression\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mematrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EMatrix\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnaive_regression\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnp_reference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_model, predict_model\n",
      "File \u001b[0;32m~/HelLibpt2/naive_regression/crypto_utils.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Tuple, Optional, List\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenfhe\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenfhe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PKESchemeFeature\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnaive_regression\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mematrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EMatrix\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openfhe.cpython-312-darwin.so, 0x0002): Library not loaded: @rpath/libOPENFHEpke.1.dylib\n  Referenced from: <5B46D50D-E5D8-3708-B922-EB28A1D0814B> /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openfhe.cpython-312-darwin.so\n  Reason: no LC_RPATH's found"
     ]
    }
   ],
   "source": [
    "# Required Imports\n",
    "import numpy as np\n",
    "from openfhe_lib.ckks.openFHE import * \n",
    "import random\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "from pprint import pprint\n",
    "from typing import Tuple\n",
    "\n",
    "import yaml\n",
    "from naive_regression.crypto_utils import setup_crypto\n",
    "from naive_regression.ematrix import EMatrix\n",
    "\n",
    "from naive_regression.np_reference import train_model, predict_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(\n",
    "        X:EMatrix,\n",
    "        weights: EMatrix,\n",
    ") -> EMatrix:\n",
    "    ################################################\n",
    "    # Exe: implement the prediction via. a dot-product.\n",
    "    #       think carefully about what the out-packing might be\n",
    "    ################################################\n",
    "    return X.dot(weights, \"vertical\")\n",
    "\n",
    "\n",
    "def calculate_loss(prediction: EMatrix, label: EMatrix,\n",
    "                   inverse_num_samples_scale: float,\n",
    "                   ) -> Tuple[EMatrix, EMatrix]:\n",
    "    residuals = label - prediction\n",
    "    # compute error (difference between estimate y_hat and true value y)\n",
    "    sq_error = residuals.hprod(residuals)\n",
    "    enc_SSE = sq_error.sum()\n",
    "\n",
    "    enc_SSE *= inverse_num_samples_scale\n",
    "    return residuals, enc_SSE\n",
    "\n",
    "\n",
    "def apply_gradient(\n",
    "        X: EMatrix,\n",
    "        weights: EMatrix,\n",
    "        residuals: EMatrix,\n",
    "        scaling: float,\n",
    "        alpha: float,\n",
    "        repeat_weights_N_times: int,\n",
    ") -> Tuple[EMatrix, EMatrix]:\n",
    "    \"\"\"\n",
    "    We return the new weights and the gradients to generate these weights\n",
    "        this is to allow us to inspect if we need to.\n",
    "    \"\"\"\n",
    "    # Internally, the dot product handles the need for the transpose.\n",
    "\n",
    "    grad = X.dot(residuals, \"vertical\")\n",
    "    grad = grad * -2 * scaling\n",
    "\n",
    "    grad_alpha = grad * alpha\n",
    "    repeated_grad_alpha = grad_alpha.vecConv2Hrep(repeat_weights_N_times)\n",
    "    weights = weights - repeated_grad_alpha\n",
    "    return weights, grad\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open(\"naive_regression/config.yml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"ML Config:\")\n",
    "    pprint(config[\"ml_params\"])\n",
    "    print(\"Crypto Params:\")\n",
    "    pprint(config[\"crypto_params\"])\n",
    "    if config[\"crypto_params\"][\"run_bootstrap\"]:\n",
    "        print(\"Running with bootstrap\")\n",
    "        pprint(config[\"crypto_bootstrap_params\"])\n",
    "    ml_conf = config[\"ml_params\"]\n",
    "    batch_size = ml_conf[\"batch_size\"]\n",
    "    lr = ml_conf[\"lr\"]\n",
    "    epochs = ml_conf[\"epochs\"]\n",
    "\n",
    "    ################################################\n",
    "    # Generate data\n",
    "    ################################################\n",
    "\n",
    "    X = np.random.rand(batch_size * 5, 5)\n",
    "    y = (np.dot(X, np.random.rand(5, 1))) + np.random.rand(1)\n",
    "    noise = np.random.randn(y.shape[0], y.shape[1])\n",
    "    y = y + noise\n",
    "\n",
    "    weights = np.random.rand(5, 1)\n",
    "    print(\"#\" * 10)\n",
    "    print(\"Plaintext Performance\")\n",
    "    m_stat = train(X, y, weights, lr, epochs)\n",
    "\n",
    "    print(\"#\" * 10)\n",
    "    print(\"Encrypted Performance\")\n",
    "\n",
    "    setup_crypto(\n",
    "        num_data_points=-1 if config[\"crypto_params\"][\"run_bootstrap\"] else len(X),\n",
    "        c_params=config[\"crypto_params\"],\n",
    "        bootstrap_params=config[\"crypto_bootstrap_params\"]\n",
    "    )\n",
    "\n",
    "    inverse_scale = 1 / len(y)\n",
    "\n",
    "    ####################################################################\n",
    "    # We need to repeat the weights N-times bc we do the hadamard product then sum\n",
    "    #   when we're doing the dot product\n",
    "    weights = np.squeeze(weights, axis=1).tolist()\n",
    "    repeated_weights = []\n",
    "    for i in range(len(X)):\n",
    "        repeated_weights.append(weights)\n",
    "    weights = EMatrix.fromList(repeated_weights, packing=\"vertical\", repeated=True)\n",
    "    weights.encryptSelf()\n",
    "\n",
    "    ####################################################################\n",
    "    # We encrypt all at once. NOTE: this is not a true SGD - we're not shuffling\n",
    "    #   between each epoch. Having said that, this is WAY faster\n",
    "    e_X = EMatrix.fromList(X.tolist())\n",
    "    e_y = EMatrix.fromList(y.tolist())\n",
    "    e_X.encryptSelf()\n",
    "    e_y.encryptSelf()\n",
    "    run_bootstrap_mode = config[\"crypto_params\"][\"run_bootstrap\"]\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = predict(e_X, weights)\n",
    "        residuals, loss = calculate_loss(y_pred, label=e_y, inverse_num_samples_scale=inverse_scale)\n",
    "        weights, grads = apply_gradient(e_X, weights, residuals, inverse_scale, lr, len(X))\n",
    "\n",
    "        ################################################\n",
    "        # Exe: it's not always realistic, but you may wish to displaty the loss\n",
    "        ################################################\n",
    "\n",
    "        print(f\"epoch: {epoch} ----> MSE: {loss.decryptSelf()[0]}\")\n",
    "\n",
    "        ################################################\n",
    "        # Exe: Our ciphertexts accumulate noise as we do computations. We have two options to handle the noise:\n",
    "        #   - bootstrapping, which is expensive\n",
    "        #   - decrypting and re-encrypting, which comes with its own tradeoffs\n",
    "        #   Benchmark the two to get a feel for the timing difference\n",
    "        ################################################\n",
    "\n",
    "\n",
    "        if run_bootstrap_mode:\n",
    "            weights.bootstrap_self()\n",
    "        else:\n",
    "            weights.recrypt_self()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
